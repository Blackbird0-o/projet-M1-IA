{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbUFxk5L-jE0"
   },
   "source": [
    "# DRIVE MOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNPthfD396hf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O4DCDqXN-gsq"
   },
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "id": "Z_d6cJdY-OPS",
    "outputId": "d0b6dd8c-58ef-4551-f12c-a95e86c912c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, BatchNormalization, CuDNNLSTM, LSTM, Conv1D,UpSampling1D, MaxPool1D,MaxPooling1D, Permute, Reshape\n",
    "from keras.optimizers import RMSprop, adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import pywt\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.base import BaseSampler\n",
    "from collections import Counter # counts the number of elements per class ({0: 5050, 1: 37})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxq8Xdhw-bNr"
   },
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPQseyW3-P4g"
   },
   "outputs": [],
   "source": [
    "def RPN(x):\n",
    "    '''\n",
    "    Calcule la RPN d'un signal (Relative Power Noise)\n",
    "    input :\n",
    "        x = array numpy, le signal dont on souhaite calculer la RPN\n",
    "        \n",
    "    output :\n",
    "        x_RPN = array numpy, la RPN du signal\n",
    "        '''\n",
    "    mean = np.mean(x,axis=1).reshape(x.shape[0],1)\n",
    "    return (x-mean)/mean\n",
    "  \n",
    "def shuffle(x,y):\n",
    "    # shuffle\n",
    "    index = np.arange(y.shape[0])\n",
    "    np.random.shuffle(index)\n",
    "    x = x[index]\n",
    "    y = y[index]\n",
    "    \n",
    "    return x,y\n",
    "    \n",
    "def bootstrap(x_train,y_train,inv=True) :\n",
    "    if inv :\n",
    "      x_train,y_train = inv_data(x_train,y_train)\n",
    "      \n",
    "    x_train1 = x_train[np.where(y_train == 1)[0]] #Separation du train_set selon le label\n",
    "    x_train0 = x_train[np.where(y_train == 0)[0]]\n",
    "    index_train = np.random.randint(0,x_train1.shape[0] , size=x_train0.shape[0]) #genere une liste d'index \n",
    "                                                                                  #aléatoire pour equilibrer les données\n",
    "    x_train_1_boot = x_train1[index_train]\n",
    "    y_train_boot = np.concatenate((np.ones(x_train0.shape[0]),np.zeros(x_train0.shape[0]))) #on génère une liste de labels avec autant de 1 que de 0\n",
    "    x_train_boot = np.concatenate((x_train_1_boot,x_train0)) #on rassemble les données une fois équilibrées\n",
    "    \n",
    "    x_train_boot,y_train_boot  = shuffle(x_train_boot,y_train_boot)\n",
    "    \n",
    "    return x_train_boot,y_train_boot\n",
    "\n",
    "def dataload(path='data/',merge=True) :\n",
    "    # Loading datas\n",
    "    data_train = pd.read_csv(path+'exoTrain.csv')\n",
    "    data_test = pd.read_csv(path+'exoTest.csv')\n",
    "    \n",
    "    # transformation des label en array de 0 et 1\n",
    "    y_train = np.array(data_train[\"LABEL\"])-1\n",
    "    y_test = np.array(data_test['LABEL'])-1\n",
    "    \n",
    "    # on charge les features\n",
    "    x_train = np.array(data_train.drop('LABEL',axis=1))\n",
    "    x_test = np.array(data_test.drop('LABEL',axis=1))\n",
    "    \n",
    "    if merge :\n",
    "      data = np.concatenate((x_train,x_test))\n",
    "      y = np.concatenate((y_train,y_test))\n",
    "      data0 = data[np.where(y==0)[0]]\n",
    "      y0 = y[np.where(y==0)[0]]\n",
    "      data1 = data[np.where(y==1)[0]]\n",
    "      y1 = y[np.where(y==1)[0]]\n",
    "      \n",
    "      x_train0,x_test0,y_train0,y_test0 = train_test_split(data0,y0, test_size = 0.1)\n",
    "      x_train1,x_test1,y_train1,y_test1 = train_test_split(data1,y1, test_size = 0.1)\n",
    "      \n",
    "      x_train = np.concatenate((x_train0,x_train1))\n",
    "      y_train = np.concatenate((y_train0,y_train1))\n",
    "      x_test = np.concatenate((x_test0,x_test1))\n",
    "      y_test = np.concatenate((y_test0,y_test1))\n",
    "      \n",
    "      x_train,y_train = shuffle(x_train,y_train)\n",
    "      x_test,y_test = shuffle(x_test,y_test)\n",
    "    \n",
    "    return x_train,y_train,x_test,y_test\n",
    "\n",
    "def pcaPlot(X, y, descr= 'temporel',plot_samples = 500):\n",
    "  '''\n",
    "  Defines and 10 components PCA of the dataset X and plots the first 3\n",
    "  '''\n",
    "  pca = PCA(n_components=10)\n",
    "  x_PCA = pca.fit_transform(X)\n",
    "\n",
    "  # let's visualize the data in 3d\n",
    "  fig = plt.figure()\n",
    "  ax = fig.add_subplot(111, projection='3d')\n",
    "  ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "  ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "  ax.set_zlabel('Principal Component 3', fontsize = 15)\n",
    "  ax.set_title('ACP du signal ' + descr, fontsize = 20)\n",
    "  targets = [0,1]\n",
    "  colors = ['b', 'r']\n",
    "  x_PCA_plot = x_PCA[0:plot_samples]\n",
    "\n",
    "  for target, color in zip(targets,colors):\n",
    "      indexes = np.where(y[0:plot_samples] == target)\n",
    "      ax.scatter(x_PCA_plot[indexes,0]\n",
    "                , x_PCA_plot[indexes,1],\n",
    "                x_PCA_plot[indexes,2]\n",
    "                , c = color\n",
    "                , s = 50)\n",
    "  ax.legend(['pas d\\'exoplanetes', 'exoplanetes'])\n",
    "  ax.grid()\n",
    "  plt.show()\n",
    "  return None\n",
    "\n",
    "#Make an identity sampler\n",
    "class FakeSampler(BaseSampler):\n",
    "\n",
    "    _sampling_type = 'bypass'\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        return X, y\n",
    "\n",
    "def plot_resampling(X, y, sampling, ax):\n",
    "    X_res, y_res = sampling.fit_resample(X, y)\n",
    "    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor='k')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines['left'].set_position(('outward', 10))\n",
    "    ax.spines['bottom'].set_position(('outward', 10))\n",
    "    return Counter(y_res)\n",
    "\n",
    "def SMOTE_plot(x_train, y_train):\n",
    "  sampler = FakeSampler()\n",
    "\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n",
    "  plot_resampling(x_train, y_train, sampler, ax1)\n",
    "  ax1.set_title('Original data - y={}'.format(Counter(y_train)))\n",
    "\n",
    "  plot_resampling(x_train, y_train, SMOTE(random_state = 0), ax2)\n",
    "  ax2.set_title('Resampling using {}'.format(SMOTE(random_state=0).__class__.__name__))\n",
    "  fig.tight_layout()\n",
    "  plt.show()\n",
    "  return None\n",
    "\n",
    "def transform_dataset(X, mode='wavelet', wname='db5',nsamples=10):\n",
    "  if mode == 'wavelet':\n",
    "    return pywt.dwt(X, wname)[0][:,0:nsamples]\n",
    "\n",
    "  elif mode == 'fft':\n",
    "    return np.abs(np.fft.fft(X))[:,0:nsamples]\n",
    "\n",
    "  elif mode == 'all_in':\n",
    "    allz = np.abs(np.fft.fft(X))[:,0:nsamples]\n",
    "    wnames = ['db5','sym5','coif5','bior2.4']\n",
    "    for wn in wnames:\n",
    "      np.append(allz, pywt.dwt(X, wn)[0][:,0:nsamples], axis=1)\n",
    "    return allz\n",
    "\n",
    "def scale_datasets(X_train, X_test, param='standardScaling', reshape=True):\n",
    "  SC = StandardScaler()\n",
    "  train_shape = X_train.shape\n",
    "  test_shape = X_test.shape\n",
    "    \n",
    "  if param == 'standardScaling':\n",
    "    SC.fit(X_train)\n",
    "    if reshape:\n",
    "      return SC.transform(X_train).reshape(train_shape[0],train_shape[1],1), SC.transform(X_test).reshape(test_shape[0],test_shape[1],1)\n",
    "    else :\n",
    "      return SC.transform(X_train), SC.transform(X_test)\n",
    "\n",
    "  elif param == 'RPN':\n",
    "    \n",
    "    mean_train = np.mean(X_train,axis=1).reshape(X_train.shape[0],1)\n",
    "    mean_test = np.mean(X_test,axis=1).reshape(X_test.shape[0],1)\n",
    "    \n",
    "    norm_train = np.max(np.abs(X_train),axis=1).reshape(-1,1)#np.linalg.norm(X_train,axis=1).reshape(-1,1)\n",
    "    norm_test = np.max(np.abs(X_test),axis=1).reshape(-1,1)#np.linalg.norm(X_test,axis=1).reshape(-1,1)\n",
    "    \n",
    "    if reshape:\n",
    "      return ((X_train-mean_train)/norm_train) .reshape(train_shape[0],train_shape[1],1) , ((X_test-mean_test)/norm_test) .reshape(test_shape[0],test_shape[1],1)\n",
    "    else :\n",
    "      return ((X_train-mean_train)/norm_train)  , ((X_test-mean_test)/norm_test) \n",
    "    \n",
    "    \n",
    "  elif param == 'transpose':\n",
    "    X_train = np.transpose(X_train)\n",
    "    if train_shape != test_shape :\n",
    "      X_test = np.tile(X_test,(10,1))[0:train_shape[0]]\n",
    "    X_test = np.transpose(X_test)\n",
    "    SC.fit(X_train)\n",
    "    if reshape:\n",
    "      return np.transpose(SC.transform(X_train)).reshape(train_shape[0],train_shape[1],1), np.transpose(SC.transform(X_test))[0:test_shape[0]].reshape(test_shape[0],test_shape[1],1)\n",
    "    else :\n",
    "      return np.transpose(SC.transform(X_train)), np.transpose(SC.transform(X_test))[0:test_shape[0]]\n",
    "    \n",
    "  elif param == 'flatten':\n",
    "    X_train = X_train.flatten().reshape((-1,1))\n",
    "    X_test = X_test.flatten().reshape((-1,1))\n",
    "    SC.fit(X_train)\n",
    "    if reshape:\n",
    "      return SC.transform(X_train).reshape(train_shape[0],train_shape[1],1), SC.transform(X_test).reshape(test_shape[0],test_shape[1],1)\n",
    "    else :\n",
    "      return SC.transform(X_train).reshape(train_shape[0],train_shape[1]), SC.transform(X_test).reshape(test_shape[0],test_shape[1])\n",
    "  \n",
    "  elif param == 'norm':\n",
    "    norm_train = np.linalg.norm(X_train,axis=1).reshape(-1,1)\n",
    "    norm_test = np.linalg.norm(X_test,axis=1).reshape(-1,1)\n",
    "    if reshape:\n",
    "      return (X_train/norm_train).reshape(train_shape[0],train_shape[1],1), (X_test/norm_test).reshape(test_shape[0],test_shape[1],1)\n",
    "    else :\n",
    "      return X_train/norm_train, X_test/norm_test\n",
    "    \n",
    "  elif param == 'norm_flatten':\n",
    "    norm_train = np.linalg.norm(X_train)\n",
    "    norm_test = np.linalg.norm(X_test,axis=1).reshape(-1,1)\n",
    "    if reshape:\n",
    "      return (X_train/norm_train).reshape(train_shape[0],train_shape[1],1), (X_test/norm_train).reshape(test_shape[0],test_shape[1],1)\n",
    "    else :\n",
    "      return X_train/norm_train, X_test/norm_train\n",
    "\n",
    "def inv_data(X, y):\n",
    "  X_flipped = np.flip(X[np.where(y == 1)[0]], 1)\n",
    "  y_flipped = np.ones((X_flipped.shape[0]))\n",
    "  return np.concatenate((X, X_flipped)), np.concatenate((y, y_flipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBXv4ImW_60C"
   },
   "source": [
    "# METRICS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YALCfl9M_-KW"
   },
   "outputs": [],
   "source": [
    "def getScores(pred, result):\n",
    "  print('Precision :')\n",
    "  print(precision_score(result, pred))\n",
    "\n",
    "  print('Recall :')\n",
    "  print(recall_score(result, pred))\n",
    "\n",
    "  print('F1 Score :')\n",
    "  scoref1 = f1_score(result, pred)\n",
    "  print(scoref1)\n",
    "\n",
    "  print('MSE :')\n",
    "  modelError = mean_squared_error(result, pred)\n",
    "  print(modelError)\n",
    "\n",
    "  print('')\n",
    "  print('confusion_matrix : ')\n",
    "  confusion = confusion_matrix(result, pred)\n",
    "  print(confusion)\n",
    "  print('')\n",
    "  return scoref1, modelError, confusion\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "  \"\"\"Recall metric.\n",
    "  Only computes a batch-wise average of recall.\n",
    "  Computes the recall, a metric for multi-label classification of\n",
    "  how many relevant items are selected.\n",
    "  \"\"\"\n",
    "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "  return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "  \"\"\"Precision metric.\n",
    "  Only computes a batch-wise average of precision.\n",
    "  Computes the precision, a metric for multi-label classification of\n",
    "  how many selected items are relevant.\n",
    "  \"\"\"\n",
    "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "  return true_positives / (predicted_positives + K.epsilon())\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "  preci = precision(y_true, y_pred)\n",
    "  rec = recall(y_true, y_pred)\n",
    "  return 2*((preci*rec)/(preci+rec+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tl4HT84oAOgo"
   },
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQiMkUnUAQK6"
   },
   "outputs": [],
   "source": [
    "def maxinet(x_train,y_train,x_test,y_test):\n",
    "  #X = X.reshape(-1,1)\n",
    "  #X_tst = X_tst.reshape(-1,1)\n",
    "  model = Sequential()\n",
    "\n",
    "  \n",
    "  model.add(Conv1D(16, 200, activation='relu', padding='same', input_shape=x_train.shape[1:]))\n",
    "  model.add(MaxPooling1D(4, padding='same'))\n",
    "  model.add(Conv1D(8, 100, activation='relu', padding='same'))\n",
    "  model.add(MaxPooling1D(4, padding='same'))\n",
    "  model.add(Conv1D(4, 10, activation='relu', padding='same'))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(CuDNNLSTM(200, return_sequences=True))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(CuDNNLSTM(70, return_sequences=True))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(CuDNNLSTM(10))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "  \n",
    "  '''\n",
    "  autoencoder.add(UpSampling1D(4))\n",
    "  autoencoder.add(Conv1D(1, 4, activation='sigmoid', padding='same'))\n",
    "  '''\n",
    "  model.summary()\n",
    "\n",
    "\n",
    "\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[precision]) #[f1, precision, \"accuracy\"]\n",
    "  model.fit(x_train, y_train,\n",
    "                  epochs=5,\n",
    "                  batch_size=32)\n",
    "  \n",
    "\n",
    "\n",
    "  return model, np.rint(model.predict(x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ml5ft1czAcVD"
   },
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D83iTNYAAobP"
   },
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_hI6h7wmAeRg"
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = dataload(merge=True,path='data/')\n",
    "\n",
    "x_train, y_train = bootstrap(x_train, y_train)\n",
    "x_test, y_test = bootstrap(x_test, y_test,inv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ha_DNHRlBzFZ"
   },
   "outputs": [],
   "source": [
    "x_train, x_test = scale_datasets(x_train, x_test, param='RPN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPokCbCXAyJg"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1anG1r8sA0GR",
    "outputId": "5aa7fe01-77ca-4dc5-b2e7-50c88542a1e0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1129 14:18:12.864346 139731064538944 deprecation_wrapper.py:119] From /home/IA/.conda/envs/IA/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1129 14:18:12.881837 139731064538944 deprecation_wrapper.py:119] From /home/IA/.conda/envs/IA/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1129 14:18:12.884921 139731064538944 deprecation_wrapper.py:119] From /home/IA/.conda/envs/IA/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1129 14:18:12.903996 139731064538944 deprecation_wrapper.py:119] From /home/IA/.conda/envs/IA/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1129 14:18:13.024567 139731064538944 deprecation_wrapper.py:119] From /home/IA/.conda/envs/IA/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1129 14:18:13.037595 139731064538944 deprecation.py:506] From /home/IA/.conda/envs/IA/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1129 14:18:13.890288 139731064538944 deprecation_wrapper.py:119] From /home/IA/.conda/envs/IA/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1129 14:18:13.906371 139731064538944 deprecation_wrapper.py:119] From /home/IA/.conda/envs/IA/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W1129 14:18:13.910633 139731064538944 deprecation.py:323] From /home/IA/.conda/envs/IA/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3197, 16)          3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 800, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 800, 8)            12808     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 200, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 200, 4)            324       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200, 4)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 200, 200)          164800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200, 200)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 200, 70)           76160     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200, 70)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)     (None, 10)                3280      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 260,599\n",
      "Trainable params: 260,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b5677bda99d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaxinet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgetScores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8be874e2d803>\u001b[0m in \u001b[0;36mmaxinet\u001b[0;34m(x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     31\u001b[0m   model.fit(x_train, y_train,\n\u001b[1;32m     32\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                   batch_size=32)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/IA/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.conda/envs/IA/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/IA/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/IA/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    184\u001b[0m                 config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n\u001b[1;32m    185\u001b[0m                                         allow_soft_placement=True)\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0m_SESSION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/IA/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m   1568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \"\"\"\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m     \u001b[0;31m# NOTE(mrry): Create these on first `__enter__` to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_graph_context_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/IA/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target, graph, config)\u001b[0m\n\u001b[1;32m    691\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_NewSessionRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: CUDA runtime implicit initialization on GPU:0 failed. Status: out of memory"
     ]
    }
   ],
   "source": [
    "model, pred = maxinet(x_train,y_train,x_test,y_test)\n",
    "\n",
    "getScores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yuDpoewsH_qB"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "All stars with at least one exoplanet orbiting them have been classified as such and only a few of the non-exoplanet stars have been miss-classified. This is actually the best situation because one wouldn't want the opposite. Matter of fact, the other way around would be problematic because one would miss a lot of the stars one would like to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mtpQGQQIrX9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "O4DCDqXN-gsq",
    "sBXv4ImW_60C"
   ],
   "name": "F_Nbook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
