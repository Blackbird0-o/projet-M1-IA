{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "F_Nbook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LbUFxk5L-jE0"
      },
      "source": [
        "# DRIVE MOUNT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jNPthfD396hf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O4DCDqXN-gsq"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "colab_type": "code",
        "id": "Z_d6cJdY-OPS",
        "outputId": "bd1b438c-13ac-427c-98b0-3284156441cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten, BatchNormalization, CuDNNLSTM, LSTM, Conv1D,UpSampling1D, MaxPool1D,MaxPooling1D, Permute, Reshape\n",
        "from keras.optimizers import RMSprop, adam\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import pywt\n",
        "import pandas as pd\n",
        "from matplotlib import cm\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.base import BaseSampler\n",
        "from collections import Counter # counts the number of elements per class ({0: 5050, 1: 37})\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pxq8Xdhw-bNr"
      },
      "source": [
        "# DATA PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JPQseyW3-P4g"
      },
      "outputs": [],
      "source": [
        "def RPN(x):\n",
        "    '''\n",
        "    Calcule la RPN d'un signal (Relative Power Noise)\n",
        "    input :\n",
        "        x = array numpy, le signal dont on souhaite calculer la RPN\n",
        "        \n",
        "    output :\n",
        "        x_RPN = array numpy, la RPN du signal\n",
        "        '''\n",
        "    mean = np.mean(x,axis=1).reshape(x.shape[0],1)\n",
        "    return (x-mean)/mean\n",
        "  \n",
        "def shuffle(x,y):\n",
        "    # shuffle\n",
        "    index = np.arange(y.shape[0])\n",
        "    np.random.shuffle(index)\n",
        "    x = x[index]\n",
        "    y = y[index]\n",
        "    \n",
        "    return x,y\n",
        "    \n",
        "def bootstrap(x_train,y_train,inv=True) :\n",
        "    if inv :\n",
        "      x_train,y_train = inv_data(x_train,y_train)\n",
        "      \n",
        "    x_train1 = x_train[np.where(y_train == 1)[0]] #Separation du train_set selon le label\n",
        "    x_train0 = x_train[np.where(y_train == 0)[0]]\n",
        "    index_train = np.random.randint(0,x_train1.shape[0] , size=x_train0.shape[0]) #genere une liste d'index \n",
        "                                                                                  #aléatoire pour equilibrer les données\n",
        "    x_train_1_boot = x_train1[index_train]\n",
        "    y_train_boot = np.concatenate((np.ones(x_train0.shape[0]),np.zeros(x_train0.shape[0]))) #on génère une liste de labels avec autant de 1 que de 0\n",
        "    x_train_boot = np.concatenate((x_train_1_boot,x_train0)) #on rassemble les données une fois équilibrées\n",
        "    \n",
        "    x_train_boot,y_train_boot  = shuffle(x_train_boot,y_train_boot)\n",
        "    \n",
        "    return x_train_boot,y_train_boot\n",
        "\n",
        "def dataload(path='data/',merge=True) :\n",
        "    # Loading datas\n",
        "    data_train = pd.read_csv(path+'exoTrain.csv')\n",
        "    data_test = pd.read_csv(path+'exoTest.csv')\n",
        "    \n",
        "    # transformation des label en array de 0 et 1\n",
        "    y_train = np.array(data_train[\"LABEL\"])-1\n",
        "    y_test = np.array(data_test['LABEL'])-1\n",
        "    \n",
        "    # on charge les features\n",
        "    x_train = np.array(data_train.drop('LABEL',axis=1))\n",
        "    x_test = np.array(data_test.drop('LABEL',axis=1))\n",
        "    \n",
        "    if merge :\n",
        "      data = np.concatenate((x_train,x_test))\n",
        "      y = np.concatenate((y_train,y_test))\n",
        "      data0 = data[np.where(y==0)[0]]\n",
        "      y0 = y[np.where(y==0)[0]]\n",
        "      data1 = data[np.where(y==1)[0]]\n",
        "      y1 = y[np.where(y==1)[0]]\n",
        "      \n",
        "      x_train0,x_test0,y_train0,y_test0 = train_test_split(data0,y0, test_size = 0.1)\n",
        "      x_train1,x_test1,y_train1,y_test1 = train_test_split(data1,y1, test_size = 0.1)\n",
        "      \n",
        "      x_train = np.concatenate((x_train0,x_train1))\n",
        "      y_train = np.concatenate((y_train0,y_train1))\n",
        "      x_test = np.concatenate((x_test0,x_test1))\n",
        "      y_test = np.concatenate((y_test0,y_test1))\n",
        "      \n",
        "      x_train,y_train = shuffle(x_train,y_train)\n",
        "      x_test,y_test = shuffle(x_test,y_test)\n",
        "    \n",
        "    return x_train,y_train,x_test,y_test\n",
        "\n",
        "def pcaPlot(X, y, descr= 'temporel',plot_samples = 500):\n",
        "  '''\n",
        "  Defines and 10 components PCA of the dataset X and plots the first 3\n",
        "  '''\n",
        "  pca = PCA(n_components=10)\n",
        "  x_PCA = pca.fit_transform(X)\n",
        "\n",
        "  # let's visualize the data in 3d\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "  ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "  ax.set_zlabel('Principal Component 3', fontsize = 15)\n",
        "  ax.set_title('ACP du signal ' + descr, fontsize = 20)\n",
        "  targets = [0,1]\n",
        "  colors = ['b', 'r']\n",
        "  x_PCA_plot = x_PCA[0:plot_samples]\n",
        "\n",
        "  for target, color in zip(targets,colors):\n",
        "      indexes = np.where(y[0:plot_samples] == target)\n",
        "      ax.scatter(x_PCA_plot[indexes,0]\n",
        "                , x_PCA_plot[indexes,1],\n",
        "                x_PCA_plot[indexes,2]\n",
        "                , c = color\n",
        "                , s = 50)\n",
        "  ax.legend(['pas d\\'exoplanetes', 'exoplanetes'])\n",
        "  ax.grid()\n",
        "  plt.show()\n",
        "  return None\n",
        "\n",
        "#Make an identity sampler\n",
        "class FakeSampler(BaseSampler):\n",
        "\n",
        "    _sampling_type = 'bypass'\n",
        "\n",
        "    def _fit_resample(self, X, y):\n",
        "        return X, y\n",
        "\n",
        "def plot_resampling(X, y, sampling, ax):\n",
        "    X_res, y_res = sampling.fit_resample(X, y)\n",
        "    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor='k')\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.get_xaxis().tick_bottom()\n",
        "    ax.get_yaxis().tick_left()\n",
        "    ax.spines['left'].set_position(('outward', 10))\n",
        "    ax.spines['bottom'].set_position(('outward', 10))\n",
        "    return Counter(y_res)\n",
        "\n",
        "def SMOTE_plot(x_train, y_train):\n",
        "  sampler = FakeSampler()\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n",
        "  plot_resampling(x_train, y_train, sampler, ax1)\n",
        "  ax1.set_title('Original data - y={}'.format(Counter(y_train)))\n",
        "\n",
        "  plot_resampling(x_train, y_train, SMOTE(random_state = 0), ax2)\n",
        "  ax2.set_title('Resampling using {}'.format(SMOTE(random_state=0).__class__.__name__))\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "  return None\n",
        "\n",
        "def transform_dataset(X, mode='wavelet', wname='db5',nsamples=10):\n",
        "  if mode == 'wavelet':\n",
        "    return pywt.dwt(X, wname)[0][:,0:nsamples]\n",
        "\n",
        "  elif mode == 'fft':\n",
        "    return np.abs(np.fft.fft(X))[:,0:nsamples]\n",
        "\n",
        "  elif mode == 'all_in':\n",
        "    allz = np.abs(np.fft.fft(X))[:,0:nsamples]\n",
        "    wnames = ['db5','sym5','coif5','bior2.4']\n",
        "    for wn in wnames:\n",
        "      np.append(allz, pywt.dwt(X, wn)[0][:,0:nsamples], axis=1)\n",
        "    return allz\n",
        "\n",
        "def scale_datasets(X_train, X_test, param='standardScaling', reshape=True):\n",
        "  SC = StandardScaler()\n",
        "  train_shape = X_train.shape\n",
        "  test_shape = X_test.shape\n",
        "    \n",
        "  if param == 'standardScaling':\n",
        "    SC.fit(X_train)\n",
        "    if reshape:\n",
        "      return SC.transform(X_train).reshape(train_shape[0],train_shape[1],1), SC.transform(X_test).reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return SC.transform(X_train), SC.transform(X_test)\n",
        "\n",
        "  elif param == 'RPN':\n",
        "    \n",
        "    mean_train = np.mean(X_train,axis=1).reshape(X_train.shape[0],1)\n",
        "    mean_test = np.mean(X_test,axis=1).reshape(X_test.shape[0],1)\n",
        "    \n",
        "    norm_train = np.max(np.abs(X_train),axis=1).reshape(-1,1)#np.linalg.norm(X_train,axis=1).reshape(-1,1)\n",
        "    norm_test = np.max(np.abs(X_test),axis=1).reshape(-1,1)#np.linalg.norm(X_test,axis=1).reshape(-1,1)\n",
        "    \n",
        "    if reshape:\n",
        "      return ((X_train-mean_train)/norm_train) .reshape(train_shape[0],train_shape[1],1) , ((X_test-mean_test)/norm_test) .reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return ((X_train-mean_train)/norm_train)  , ((X_test-mean_test)/norm_test) \n",
        "    \n",
        "    \n",
        "  elif param == 'transpose':\n",
        "    X_train = np.transpose(X_train)\n",
        "    if train_shape != test_shape :\n",
        "      X_test = np.tile(X_test,(10,1))[0:train_shape[0]]\n",
        "    X_test = np.transpose(X_test)\n",
        "    SC.fit(X_train)\n",
        "    if reshape:\n",
        "      return np.transpose(SC.transform(X_train)).reshape(train_shape[0],train_shape[1],1), np.transpose(SC.transform(X_test))[0:test_shape[0]].reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return np.transpose(SC.transform(X_train)), np.transpose(SC.transform(X_test))[0:test_shape[0]]\n",
        "    \n",
        "  elif param == 'flatten':\n",
        "    X_train = X_train.flatten().reshape((-1,1))\n",
        "    X_test = X_test.flatten().reshape((-1,1))\n",
        "    SC.fit(X_train)\n",
        "    if reshape:\n",
        "      return SC.transform(X_train).reshape(train_shape[0],train_shape[1],1), SC.transform(X_test).reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return SC.transform(X_train).reshape(train_shape[0],train_shape[1]), SC.transform(X_test).reshape(test_shape[0],test_shape[1])\n",
        "  \n",
        "  elif param == 'norm':\n",
        "    norm_train = np.linalg.norm(X_train,axis=1).reshape(-1,1)\n",
        "    norm_test = np.linalg.norm(X_test,axis=1).reshape(-1,1)\n",
        "    if reshape:\n",
        "      return (X_train/norm_train).reshape(train_shape[0],train_shape[1],1), (X_test/norm_test).reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return X_train/norm_train, X_test/norm_test\n",
        "    \n",
        "  elif param == 'norm_flatten':\n",
        "    norm_train = np.linalg.norm(X_train)\n",
        "    norm_test = np.linalg.norm(X_test,axis=1).reshape(-1,1)\n",
        "    if reshape:\n",
        "      return (X_train/norm_train).reshape(train_shape[0],train_shape[1],1), (X_test/norm_train).reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return X_train/norm_train, X_test/norm_train\n",
        "\n",
        "def inv_data(X, y):\n",
        "  X_flipped = np.flip(X[np.where(y == 1)[0]], 1)\n",
        "  y_flipped = np.ones((X_flipped.shape[0]))\n",
        "  return np.concatenate((X, X_flipped)), np.concatenate((y, y_flipped))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sBXv4ImW_60C"
      },
      "source": [
        "# METRICS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YALCfl9M_-KW"
      },
      "outputs": [],
      "source": [
        "def getScores(pred, result):\n",
        "  print('Precision :')\n",
        "  print(precision_score(result, pred))\n",
        "\n",
        "  print('Recall :')\n",
        "  print(recall_score(result, pred))\n",
        "\n",
        "  print('F1 Score :')\n",
        "  scoref1 = f1_score(result, pred)\n",
        "  print(scoref1)\n",
        "\n",
        "  print('MSE :')\n",
        "  modelError = mean_squared_error(result, pred)\n",
        "  print(modelError)\n",
        "\n",
        "  print('')\n",
        "  print('confusion_matrix : ')\n",
        "  confusion = confusion_matrix(result, pred)\n",
        "  print(confusion)\n",
        "  print('')\n",
        "  return scoref1, modelError, confusion\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "  \"\"\"Recall metric.\n",
        "  Only computes a batch-wise average of recall.\n",
        "  Computes the recall, a metric for multi-label classification of\n",
        "  how many relevant items are selected.\n",
        "  \"\"\"\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "  return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "  \"\"\"Precision metric.\n",
        "  Only computes a batch-wise average of precision.\n",
        "  Computes the precision, a metric for multi-label classification of\n",
        "  how many selected items are relevant.\n",
        "  \"\"\"\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "  return true_positives / (predicted_positives + K.epsilon())\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "  preci = precision(y_true, y_pred)\n",
        "  rec = recall(y_true, y_pred)\n",
        "  return 2*((preci*rec)/(preci+rec+K.epsilon()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tl4HT84oAOgo"
      },
      "source": [
        "# NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TQiMkUnUAQK6"
      },
      "outputs": [],
      "source": [
        "def maxinet(x_train,y_train,x_test,y_test):\n",
        "  #X = X.reshape(-1,1)\n",
        "  #X_tst = X_tst.reshape(-1,1)\n",
        "  model = Sequential()\n",
        "\n",
        "  \n",
        "  model.add(Conv1D(16, 200, activation='relu', padding='same', input_shape=x_train.shape[1:]))\n",
        "  model.add(MaxPooling1D(4, padding='same'))\n",
        "  model.add(Conv1D(8, 100, activation='relu', padding='same'))\n",
        "  model.add(MaxPooling1D(4, padding='same'))\n",
        "  model.add(Conv1D(4, 10, activation='relu', padding='same'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(CuDNNLSTM(200, return_sequences=True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(CuDNNLSTM(20)) \n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  \n",
        "  '''\n",
        "  autoencoder.add(UpSampling1D(4))\n",
        "  autoencoder.add(Conv1D(1, 4, activation='sigmoid', padding='same'))\n",
        "  '''\n",
        "  model.summary()\n",
        "\n",
        "\n",
        "\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[precision]) #[f1, precision, \"accuracy\"]\n",
        "  model.fit(x_train, y_train,\n",
        "                  epochs=6,\n",
        "                  shuffle = True,\n",
        "                  batch_size=32)\n",
        "  \n",
        "\n",
        "\n",
        "  return model, np.rint(model.predict(x_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ml5ft1czAcVD"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D83iTNYAAobP"
      },
      "source": [
        "## Data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_hI6h7wmAeRg"
      },
      "outputs": [],
      "source": [
        "x_train, y_train, x_test, y_test = dataload(merge=True,path='drive/My Drive/M1/IA/')\n",
        "\n",
        "x_train, y_train = bootstrap(x_train, y_train)\n",
        "x_test, y_test = bootstrap(x_test, y_test,inv=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ha_DNHRlBzFZ"
      },
      "outputs": [],
      "source": [
        "x_train, x_test = scale_datasets(x_train, x_test, param='RPN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LPokCbCXAyJg"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "1anG1r8sA0GR",
        "outputId": "e064a4e2-62cc-4264-edc0-6f89ab33ef20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_25 (Conv1D)           (None, 3197, 16)          3216      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_17 (MaxPooling (None, 800, 16)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_26 (Conv1D)           (None, 800, 8)            12808     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_18 (MaxPooling (None, 200, 8)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_27 (Conv1D)           (None, 200, 4)            324       \n",
            "_________________________________________________________________\n",
            "dropout_25 (Dropout)         (None, 200, 4)            0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_17 (CuDNNLSTM)    (None, 200, 200)          164800    \n",
            "_________________________________________________________________\n",
            "dropout_26 (Dropout)         (None, 200, 200)          0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_18 (CuDNNLSTM)    (None, 20)                17760     \n",
            "_________________________________________________________________\n",
            "dropout_27 (Dropout)         (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 198,929\n",
            "Trainable params: 198,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/6\n",
            "10106/10106 [==============================] - 31s 3ms/step - loss: 0.5854 - precision: 0.7363\n",
            "Epoch 2/6\n",
            "10106/10106 [==============================] - 27s 3ms/step - loss: 0.3131 - precision: 0.8776\n",
            "Epoch 3/6\n",
            "10106/10106 [==============================] - 27s 3ms/step - loss: 0.1920 - precision: 0.9170\n",
            "Epoch 4/6\n",
            "10106/10106 [==============================] - 27s 3ms/step - loss: 0.1399 - precision: 0.9360\n",
            "Epoch 5/6\n",
            "10106/10106 [==============================] - 27s 3ms/step - loss: 0.1354 - precision: 0.9398\n",
            "Epoch 6/6\n",
            "10106/10106 [==============================] - 27s 3ms/step - loss: 0.0687 - precision: 0.9705\n",
            "Precision :\n",
            "1.0\n",
            "Recall :\n",
            "0.9790940766550522\n",
            "F1 Score :\n",
            "0.9894366197183099\n",
            "MSE :\n",
            "0.010676156583629894\n",
            "\n",
            "confusion_matrix : \n",
            "[[550   0]\n",
            " [ 12 562]]\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.9894366197183099, 0.010676156583629894, array([[550,   0],\n",
              "        [ 12, 562]]))"
            ]
          },
          "execution_count": 24,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model, pred = maxinet(x_train,y_train,x_test,y_test)\n",
        "\n",
        "getScores(y_test, pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yuDpoewsH_qB"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "All stars with at least one exoplanet orbiting them have been classified as such and only a few of the non-exoplanet stars have been miss-classified. This is actually the best situation because one wouldn't want the opposite. Matter of fact, the other way around would be problematic because one would miss a lot of the stars one would like to study."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_mtpQGQQIrX9"
      },
      "outputs": [],
      "source": []
    }
  ]
}