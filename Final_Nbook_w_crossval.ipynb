{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Nbook_w_crossval.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pxq8Xdhw-bNr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbUFxk5L-jE0",
        "colab_type": "text"
      },
      "source": [
        "# DRIVE MOUNT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNPthfD396hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4DCDqXN-gsq",
        "colab_type": "text"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_d6cJdY-OPS",
        "colab_type": "code",
        "outputId": "c9bec87b-32ba-4b2f-cd9c-34f7741237b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Dropout, Flatten, BatchNormalization, CuDNNLSTM, LSTM, Conv1D,UpSampling1D, MaxPool1D,MaxPooling1D, Permute, Reshape\n",
        "from keras.optimizers import RMSprop, adam\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import pywt\n",
        "import pandas as pd\n",
        "from matplotlib import cm\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.base import BaseSampler\n",
        "from collections import Counter # counts the number of elements per class ({0: 5050, 1: 37})\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxq8Xdhw-bNr",
        "colab_type": "text"
      },
      "source": [
        "# DATA PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPQseyW3-P4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RPN(x):\n",
        "    '''\n",
        "    Calcule la RPN d'un signal (Relative Power Noise)\n",
        "    input :\n",
        "        x = array numpy, le signal dont on souhaite calculer la RPN\n",
        "        \n",
        "    output :\n",
        "        x_RPN = array numpy, la RPN du signal\n",
        "        '''\n",
        "    mean = np.mean(x,axis=1).reshape(x.shape[0],1)\n",
        "    return (x-mean)/mean\n",
        "  \n",
        "def shuffle(x,y):\n",
        "    # shuffle\n",
        "    index = np.arange(y.shape[0])\n",
        "    np.random.shuffle(index)\n",
        "    x = x[index]\n",
        "    y = y[index]\n",
        "    \n",
        "    return x,y\n",
        "    \n",
        "def bootstrap(x_train,y_train,inv=True) :\n",
        "    if inv :\n",
        "      x_train,y_train = inv_data(x_train,y_train)\n",
        "      \n",
        "    x_train1 = x_train[np.where(y_train == 1)[0]] #Separation du train_set selon le label\n",
        "    x_train0 = x_train[np.where(y_train == 0)[0]]\n",
        "    index_train = np.random.randint(0,x_train1.shape[0] , size=x_train0.shape[0]) #genere une liste d'index \n",
        "                                                                                  #aléatoire pour equilibrer les données\n",
        "    x_train_1_boot = x_train1[index_train]\n",
        "    y_train_boot = np.concatenate((np.ones(x_train0.shape[0]),np.zeros(x_train0.shape[0]))) #on génère une liste de labels avec autant de 1 que de 0\n",
        "    x_train_boot = np.concatenate((x_train_1_boot,x_train0)) #on rassemble les données une fois équilibrées\n",
        "    \n",
        "    x_train_boot,y_train_boot  = shuffle(x_train_boot,y_train_boot)\n",
        "    \n",
        "    return x_train_boot,y_train_boot\n",
        "\n",
        "def dataload(path='data/',merge=True) :\n",
        "    # Loading datas\n",
        "    data_train = pd.read_csv(path+'exoTrain.csv')\n",
        "    data_test = pd.read_csv(path+'exoTest.csv')\n",
        "    \n",
        "    # transformation des label en array de 0 et 1\n",
        "    y_train = np.array(data_train[\"LABEL\"])-1\n",
        "    y_test = np.array(data_test['LABEL'])-1\n",
        "    \n",
        "    # on charge les features\n",
        "    x_train = np.array(data_train.drop('LABEL',axis=1))\n",
        "    x_test = np.array(data_test.drop('LABEL',axis=1))\n",
        "    \n",
        "    if merge :\n",
        "      data = np.concatenate((x_train,x_test))\n",
        "      y = np.concatenate((y_train,y_test))\n",
        "      data0 = data[np.where(y==0)[0]]\n",
        "      y0 = y[np.where(y==0)[0]]\n",
        "      data1 = data[np.where(y==1)[0]]\n",
        "      y1 = y[np.where(y==1)[0]]\n",
        "      \n",
        "      x_train0,x_test0,y_train0,y_test0 = train_test_split(data0,y0, test_size = 0.1)\n",
        "      x_train1,x_test1,y_train1,y_test1 = train_test_split(data1,y1, test_size = 0.1)\n",
        "      \n",
        "      x_train = np.concatenate((x_train0,x_train1))\n",
        "      y_train = np.concatenate((y_train0,y_train1))\n",
        "      x_test = np.concatenate((x_test0,x_test1))\n",
        "      y_test = np.concatenate((y_test0,y_test1))\n",
        "      \n",
        "      x_train,y_train = shuffle(x_train,y_train)\n",
        "      x_test,y_test = shuffle(x_test,y_test)\n",
        "    \n",
        "    return x_train,y_train,x_test,y_test\n",
        "\n",
        "def pcaPlot(X, y, descr= 'temporel',plot_samples = 500):\n",
        "  '''\n",
        "  Defines and 10 components PCA of the dataset X and plots the first 3\n",
        "  '''\n",
        "  pca = PCA(n_components=10)\n",
        "  x_PCA = pca.fit_transform(X)\n",
        "\n",
        "  # let's visualize the data in 3d\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "  ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "  ax.set_zlabel('Principal Component 3', fontsize = 15)\n",
        "  ax.set_title('ACP du signal ' + descr, fontsize = 20)\n",
        "  targets = [0,1]\n",
        "  colors = ['b', 'r']\n",
        "  x_PCA_plot = x_PCA[0:plot_samples]\n",
        "\n",
        "  for target, color in zip(targets,colors):\n",
        "      indexes = np.where(y[0:plot_samples] == target)\n",
        "      ax.scatter(x_PCA_plot[indexes,0]\n",
        "                , x_PCA_plot[indexes,1],\n",
        "                x_PCA_plot[indexes,2]\n",
        "                , c = color\n",
        "                , s = 50)\n",
        "  ax.legend(['pas d\\'exoplanetes', 'exoplanetes'])\n",
        "  ax.grid()\n",
        "  plt.show()\n",
        "  return None\n",
        "\n",
        "#Make an identity sampler\n",
        "class FakeSampler(BaseSampler):\n",
        "\n",
        "    _sampling_type = 'bypass'\n",
        "\n",
        "    def _fit_resample(self, X, y):\n",
        "        return X, y\n",
        "\n",
        "def plot_resampling(X, y, sampling, ax):\n",
        "    X_res, y_res = sampling.fit_resample(X, y)\n",
        "    ax.scatter(X_res[:, 0], X_res[:, 1], c=y_res, alpha=0.8, edgecolor='k')\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.get_xaxis().tick_bottom()\n",
        "    ax.get_yaxis().tick_left()\n",
        "    ax.spines['left'].set_position(('outward', 10))\n",
        "    ax.spines['bottom'].set_position(('outward', 10))\n",
        "    return Counter(y_res)\n",
        "\n",
        "def SMOTE_plot(x_train, y_train):\n",
        "  sampler = FakeSampler()\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n",
        "  plot_resampling(x_train, y_train, sampler, ax1)\n",
        "  ax1.set_title('Original data - y={}'.format(Counter(y_train)))\n",
        "\n",
        "  plot_resampling(x_train, y_train, SMOTE(random_state = 0), ax2)\n",
        "  ax2.set_title('Resampling using {}'.format(SMOTE(random_state=0).__class__.__name__))\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "  return None\n",
        "\n",
        "def transform_dataset(X, mode='wavelet', wname='db5',nsamples=10):\n",
        "  if mode == 'wavelet':\n",
        "    return pywt.dwt(X, wname)[0][:,0:nsamples]\n",
        "\n",
        "  elif mode == 'fft':\n",
        "    return np.abs(np.fft.fft(X))[:,0:nsamples]\n",
        "\n",
        "  elif mode == 'all_in':\n",
        "    allz = np.abs(np.fft.fft(X))[:,0:nsamples]\n",
        "    wnames = ['db5','sym5','coif5','bior2.4']\n",
        "    for wn in wnames:\n",
        "      np.append(allz, pywt.dwt(X, wn)[0][:,0:nsamples], axis=1)\n",
        "    return allz\n",
        "\n",
        "def scale_datasets(X_train, X_test, param='standardScaling', reshape=True):\n",
        "  SC = StandardScaler()\n",
        "  train_shape = X_train.shape\n",
        "  test_shape = X_test.shape\n",
        "    \n",
        "  if param == 'standardScaling':\n",
        "    SC.fit(X_train)\n",
        "    if reshape:\n",
        "      return SC.transform(X_train).reshape(train_shape[0],train_shape[1],1), SC.transform(X_test).reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return SC.transform(X_train), SC.transform(X_test)\n",
        "\n",
        "  elif param == 'RPN':\n",
        "    \n",
        "    mean_train = np.mean(X_train,axis=1).reshape(X_train.shape[0],1)\n",
        "    mean_test = np.mean(X_test,axis=1).reshape(X_test.shape[0],1)\n",
        "    \n",
        "    norm_train = np.max(np.abs(X_train),axis=1).reshape(-1,1)#np.linalg.norm(X_train,axis=1).reshape(-1,1)\n",
        "    norm_test = np.max(np.abs(X_test),axis=1).reshape(-1,1)#np.linalg.norm(X_test,axis=1).reshape(-1,1)\n",
        "    \n",
        "    if reshape:\n",
        "      return ((X_train-mean_train)/norm_train) .reshape(train_shape[0],train_shape[1],1) , ((X_test-mean_test)/norm_test) .reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return ((X_train-mean_train)/norm_train)  , ((X_test-mean_test)/norm_test) \n",
        "    \n",
        "    \n",
        "  elif param == 'transpose':\n",
        "    X_train = np.transpose(X_train)\n",
        "    if train_shape != test_shape :\n",
        "      X_test = np.tile(X_test,(10,1))[0:train_shape[0]]\n",
        "    X_test = np.transpose(X_test)\n",
        "    SC.fit(X_train)\n",
        "    if reshape:\n",
        "      return np.transpose(SC.transform(X_train)).reshape(train_shape[0],train_shape[1],1), np.transpose(SC.transform(X_test))[0:test_shape[0]].reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return np.transpose(SC.transform(X_train)), np.transpose(SC.transform(X_test))[0:test_shape[0]]\n",
        "    \n",
        "  elif param == 'flatten':\n",
        "    X_train = X_train.flatten().reshape((-1,1))\n",
        "    X_test = X_test.flatten().reshape((-1,1))\n",
        "    SC.fit(X_train)\n",
        "    if reshape:\n",
        "      return SC.transform(X_train).reshape(train_shape[0],train_shape[1],1), SC.transform(X_test).reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return SC.transform(X_train).reshape(train_shape[0],train_shape[1]), SC.transform(X_test).reshape(test_shape[0],test_shape[1])\n",
        "  \n",
        "  elif param == 'norm':\n",
        "    norm_train = np.linalg.norm(X_train,axis=1).reshape(-1,1)\n",
        "    norm_test = np.linalg.norm(X_test,axis=1).reshape(-1,1)\n",
        "    if reshape:\n",
        "      return (X_train/norm_train).reshape(train_shape[0],train_shape[1],1), (X_test/norm_test).reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return X_train/norm_train, X_test/norm_test\n",
        "    \n",
        "  elif param == 'norm_flatten':\n",
        "    norm_train = np.linalg.norm(X_train)\n",
        "    norm_test = np.linalg.norm(X_test,axis=1).reshape(-1,1)\n",
        "    if reshape:\n",
        "      return (X_train/norm_train).reshape(train_shape[0],train_shape[1],1), (X_test/norm_train).reshape(test_shape[0],test_shape[1],1)\n",
        "    else :\n",
        "      return X_train/norm_train, X_test/norm_train\n",
        "\n",
        "def inv_data(X, y):\n",
        "  X_flipped = np.flip(X[np.where(y == 1)[0]], 1)\n",
        "  y_flipped = np.ones((X_flipped.shape[0]))\n",
        "  return np.concatenate((X, X_flipped)), np.concatenate((y, y_flipped))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBXv4ImW_60C",
        "colab_type": "text"
      },
      "source": [
        "# METRICS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YALCfl9M_-KW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getScores_cross(pred, result, display=False):\n",
        "  confusion = confusion_matrix(result, pred)\n",
        "  \n",
        "  if display:\n",
        "    print('Precision :')\n",
        "    print(precision_score(result, pred))\n",
        "    print('Recall :')\n",
        "    print(recall_score(result, pred))\n",
        "    print('F1 Score :')\n",
        "    print(f1_score(result, pred))\n",
        "    print('MSE :')\n",
        "    print('')\n",
        "    print(mean_squared_error(result, pred))\n",
        "    print('confusion_matrix : ')\n",
        "    print(confusion)\n",
        "    print('')\n",
        "  \n",
        "  return confusion \n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "  \"\"\"Recall metric.\n",
        "  Only computes a batch-wise average of recall.\n",
        "  Computes the recall, a metric for multi-label classification of\n",
        "  how many relevant items are selected.\n",
        "  \"\"\"\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "  return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "  \"\"\"Precision metric.\n",
        "  Only computes a batch-wise average of precision.\n",
        "  Computes the precision, a metric for multi-label classification of\n",
        "  how many selected items are relevant.\n",
        "  \"\"\"\n",
        "  true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "  predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "  return true_positives / (predicted_positives + K.epsilon())\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "  preci = precision(y_true, y_pred)\n",
        "  rec = recall(y_true, y_pred)\n",
        "  return 2*((preci*rec)/(preci+rec+K.epsilon()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml5ft1czAcVD",
        "colab_type": "text"
      },
      "source": [
        "# CROSS VALIDATION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D83iTNYAAobP",
        "colab_type": "text"
      },
      "source": [
        "## Data load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0D-gXb0F960",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_load_for_cross(path='drive/My Drive/M1/IA/'):\n",
        "  '''\n",
        "  Loads data into one single dataset. \n",
        "  Takes only the path as parameter and returns two numpy arrays X and y\n",
        "  '''\n",
        "\n",
        "  # Loading data\n",
        "  data_train = pd.read_csv(path+'exoTrain.csv')\n",
        "  data_test = pd.read_csv(path+'exoTest.csv')\n",
        "  \n",
        "  # Transform labels into arrays of zeros and ones\n",
        "  y_train = np.array(data_train[\"LABEL\"])-1\n",
        "  y_test = np.array(data_test['LABEL'])-1\n",
        "  \n",
        "  # Load features\n",
        "  x_train = np.array(data_train.drop('LABEL',axis=1))\n",
        "  x_test = np.array(data_test.drop('LABEL',axis=1))\n",
        "  \n",
        "  # Merge datasets\n",
        "  X = np.concatenate((x_train,x_test))\n",
        "  y = np.concatenate((y_train,y_test))\n",
        "\n",
        "  return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hI6h7wmAeRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = data_load_for_cross()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c23oHduTRNhA",
        "colab_type": "text"
      },
      "source": [
        "## Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsYRsmC4Z33p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maxinet_cross(x_train,y_train,x_test,y_test, tst=False):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv1D(16, 200, activation='relu', padding='same', input_shape=x_train.shape[1:]))\n",
        "  model.add(MaxPooling1D(4, padding='same'))\n",
        "  model.add(Conv1D(8, 100, activation='relu', padding='same'))\n",
        "  model.add(MaxPooling1D(4, padding='same'))\n",
        "  model.add(Conv1D(4, 10, activation='relu', padding='same'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(CuDNNLSTM(200, return_sequences=True))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(CuDNNLSTM(70, return_sequences=True)) \n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(CuDNNLSTM(10)) \n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy',metrics=[precision]) #[f1, precision, \"accuracy\"]\n",
        "  if tst:\n",
        "    model.fit(x_train, y_train,\n",
        "                    epochs=1,\n",
        "                    shuffle = True,\n",
        "                    batch_size=128)\n",
        "  else:\n",
        "    model.fit(x_train, y_train,\n",
        "                    epochs=6,\n",
        "                    shuffle = True,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_test, y_test))\n",
        "  \n",
        "\n",
        "  return np.rint(model.predict(x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbDeUS9lRo66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_validation(X, y, splits=5, testing=False):\n",
        "  # Separate exoplanet stars from non-exoplanet stars\n",
        "  x_stars = X[np.where(y==0)]\n",
        "  y_stars = y[np.where(y==0)]\n",
        "  x_exo = X[np.where(y==1)]\n",
        "  y_exo = y[np.where(y==1)]\n",
        "\n",
        "  # Create splits\n",
        "  kf = KFold(n_splits=splits, random_state=None, shuffle=False)\n",
        "  split_stars = kf.split(x_stars)\n",
        "  split_exo = kf.split(x_exo)\n",
        "  scores = np.zeros((splits, 2, 2))\n",
        "\n",
        "  for k in range(splits):\n",
        "    # A bit of info\n",
        "    print(\"Running split number \", k + 1)\n",
        "\n",
        "    # Define splits\n",
        "    spS = next(split_stars)\n",
        "    spE = next(split_exo)\n",
        "    idx_tra_S = spS[0]\n",
        "    idx_tst_S = spS[1]\n",
        "    idx_tra_E = spE[0]\n",
        "    idx_tst_E = spE[1]\n",
        "\n",
        "    # Create train and test sets\n",
        "    x_tra = np.concatenate((x_stars[idx_tra_S], x_exo[idx_tra_E]))\n",
        "    y_tra = np.concatenate((y_stars[idx_tra_S], y_exo[idx_tra_E]))\n",
        "    x_tst = np.concatenate((x_stars[idx_tst_S], x_exo[idx_tst_E]))\n",
        "    y_tst = np.concatenate((y_stars[idx_tst_S], y_exo[idx_tst_E]))\n",
        "\n",
        "    # Shuffle datasets\n",
        "    x_tra, y_tra = shuffle(x_tra, y_tra)\n",
        "    x_tst, y_tst = shuffle(x_tst, y_tst)\n",
        "\n",
        "    # Bootstrap datasets\n",
        "    x_tra, y_tra = bootstrap(x_tra, y_tra)\n",
        "    x_tst, y_tst = bootstrap(x_tst, y_tst, inv=False)\n",
        "\n",
        "    # Scale datasets\n",
        "    x_tra, x_tst = scale_datasets(x_tra, x_tst, param='RPN')\n",
        "\n",
        "    # Run and evaluate NN\n",
        "    pred = maxinet_cross(x_tra, y_tra, x_tst, y_tst, tst=testing)\n",
        "    scores[k] = getScores_cross(y_tst, pred)\n",
        "\n",
        "  return scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4BfIpijFzRn",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki7Gd63Hbz-o",
        "colab_type": "code",
        "outputId": "605bb52b-ca07-46c4-e18c-0f458f6531fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Scores = cross_validation(x, y, 7)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running split number  1\n",
            "Train on 9624 samples, validate on 1606 samples\n",
            "Epoch 1/6\n",
            "9624/9624 [==============================] - 20s 2ms/step - loss: 0.5181 - precision: 0.7223 - val_loss: 0.2219 - val_precision: 0.9398\n",
            "Epoch 2/6\n",
            "9624/9624 [==============================] - 14s 1ms/step - loss: 0.2651 - precision: 0.8871 - val_loss: 0.2440 - val_precision: 0.8445\n",
            "Epoch 3/6\n",
            "9624/9624 [==============================] - 14s 1ms/step - loss: 0.1642 - precision: 0.9292 - val_loss: 0.3719 - val_precision: 0.9583\n",
            "Epoch 4/6\n",
            "9624/9624 [==============================] - 14s 1ms/step - loss: 0.1063 - precision: 0.9583 - val_loss: 0.3703 - val_precision: 0.9591\n",
            "Epoch 5/6\n",
            "9624/9624 [==============================] - 14s 1ms/step - loss: 0.1093 - precision: 0.9589 - val_loss: 0.3283 - val_precision: 0.9706\n",
            "Epoch 6/6\n",
            "9624/9624 [==============================] - 14s 1ms/step - loss: 0.2344 - precision: 0.9072 - val_loss: 0.2034 - val_precision: 0.9436\n",
            "Running split number  2\n",
            "Train on 9626 samples, validate on 1604 samples\n",
            "Epoch 1/6\n",
            "9626/9626 [==============================] - 20s 2ms/step - loss: 0.5480 - precision: 0.7711 - val_loss: 0.9172 - val_precision: 0.5529\n",
            "Epoch 2/6\n",
            "9626/9626 [==============================] - 14s 1ms/step - loss: 0.3055 - precision: 0.8836 - val_loss: 0.5065 - val_precision: 0.8844\n",
            "Epoch 3/6\n",
            "9626/9626 [==============================] - 14s 1ms/step - loss: 0.2454 - precision: 0.9093 - val_loss: 0.7681 - val_precision: 0.8962\n",
            "Epoch 4/6\n",
            "9626/9626 [==============================] - 14s 1ms/step - loss: 0.1995 - precision: 0.9262 - val_loss: 0.5910 - val_precision: 0.9158\n",
            "Epoch 5/6\n",
            "9626/9626 [==============================] - 14s 1ms/step - loss: 0.1727 - precision: 0.9350 - val_loss: 0.8751 - val_precision: 0.9301\n",
            "Epoch 6/6\n",
            "9626/9626 [==============================] - 14s 1ms/step - loss: 0.1426 - precision: 0.9463 - val_loss: 0.6844 - val_precision: 0.9309\n",
            "Running split number  3\n",
            "Train on 9626 samples, validate on 1604 samples\n",
            "Epoch 1/6\n",
            "9626/9626 [==============================] - 21s 2ms/step - loss: 0.6530 - precision: 0.6150 - val_loss: 0.5971 - val_precision: 0.5685\n",
            "Epoch 2/6\n",
            "9626/9626 [==============================] - 14s 1ms/step - loss: 0.6175 - precision: 0.7118 - val_loss: 0.5297 - val_precision: 0.8183\n",
            "Epoch 3/6\n",
            "9626/9626 [==============================] - 14s 1ms/step - loss: 0.5213 - precision: 0.8341 - val_loss: 0.4024 - val_precision: 0.8254\n",
            "Epoch 4/6\n",
            "9626/9626 [==============================] - 14s 1ms/step - loss: 0.3939 - precision: 0.8941 - val_loss: 0.2386 - val_precision: 0.9363\n",
            "Epoch 5/6\n",
            "9626/9626 [==============================] - 14s 1ms/step - loss: 0.3047 - precision: 0.9239 - val_loss: 0.1517 - val_precision: 0.9509\n",
            "Epoch 6/6\n",
            "9626/9626 [==============================] - 14s 1ms/step - loss: 0.2163 - precision: 0.9465 - val_loss: 0.3672 - val_precision: 0.9619\n",
            "Running split number  4\n",
            "Train on 9626 samples, validate on 1604 samples\n",
            "Epoch 1/6\n",
            "9626/9626 [==============================] - 21s 2ms/step - loss: 0.4811 - precision: 0.7861 - val_loss: 0.4964 - val_precision: 0.8433\n",
            "Epoch 2/6\n",
            "9626/9626 [==============================] - 14s 1ms/step - loss: 0.5021 - precision: 0.7525 - val_loss: 0.7009 - val_precision: 0.8111\n",
            "Epoch 3/6\n",
            "9626/9626 [==============================] - 14s 2ms/step - loss: 0.5512 - precision: 0.8510 - val_loss: 0.6436 - val_precision: 0.8515\n",
            "Epoch 4/6\n",
            "9626/9626 [==============================] - 14s 2ms/step - loss: 0.6460 - precision: 0.5360 - val_loss: 0.6918 - val_precision: 0.0000e+00\n",
            "Epoch 5/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.6840 - precision: 0.5362 - val_loss: 0.6531 - val_precision: 0.5972\n",
            "Epoch 6/6\n",
            "9626/9626 [==============================] - 14s 2ms/step - loss: 0.3609 - precision: 0.8793 - val_loss: 0.2116 - val_precision: 0.9135\n",
            "Running split number  5\n",
            "Train on 9626 samples, validate on 1604 samples\n",
            "Epoch 1/6\n",
            "9626/9626 [==============================] - 22s 2ms/step - loss: 0.6677 - precision: 0.5815 - val_loss: 0.6011 - val_precision: 0.6607\n",
            "Epoch 2/6\n",
            "9626/9626 [==============================] - 14s 2ms/step - loss: 0.3886 - precision: 0.8389 - val_loss: 0.5434 - val_precision: 0.9233\n",
            "Epoch 3/6\n",
            "9626/9626 [==============================] - 14s 2ms/step - loss: 0.2318 - precision: 0.9297 - val_loss: 0.4096 - val_precision: 0.9162\n",
            "Epoch 4/6\n",
            "9626/9626 [==============================] - 14s 2ms/step - loss: 0.1304 - precision: 0.9538 - val_loss: 0.4725 - val_precision: 0.9315\n",
            "Epoch 5/6\n",
            "9626/9626 [==============================] - 14s 2ms/step - loss: 0.1029 - precision: 0.9639 - val_loss: 0.5549 - val_precision: 0.9769\n",
            "Epoch 6/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.1809 - precision: 0.9356 - val_loss: 0.5738 - val_precision: 0.9818\n",
            "Running split number  6\n",
            "Train on 9626 samples, validate on 1604 samples\n",
            "Epoch 1/6\n",
            "9626/9626 [==============================] - 22s 2ms/step - loss: 0.6157 - precision: 0.7050 - val_loss: 0.6852 - val_precision: 0.6130\n",
            "Epoch 2/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.6880 - precision: 0.5660 - val_loss: 0.6856 - val_precision: 0.5997\n",
            "Epoch 3/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.6867 - precision: 0.5786 - val_loss: 0.6780 - val_precision: 0.6315\n",
            "Epoch 4/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.5394 - precision: 0.7301 - val_loss: 0.4411 - val_precision: 0.8370\n",
            "Epoch 5/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.2257 - precision: 0.9243 - val_loss: 0.4425 - val_precision: 0.8749\n",
            "Epoch 6/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.2414 - precision: 0.9057 - val_loss: 0.5799 - val_precision: 0.9668\n",
            "Running split number  7\n",
            "Train on 9626 samples, validate on 1604 samples\n",
            "Epoch 1/6\n",
            "9626/9626 [==============================] - 22s 2ms/step - loss: 0.5280 - precision: 0.7795 - val_loss: 0.5368 - val_precision: 0.8259\n",
            "Epoch 2/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.4838 - precision: 0.7707 - val_loss: 0.6938 - val_precision: 0.5000\n",
            "Epoch 3/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.6892 - precision: 0.5314 - val_loss: 0.5995 - val_precision: 0.7454\n",
            "Epoch 4/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.5947 - precision: 0.7249 - val_loss: 0.4480 - val_precision: 0.8627\n",
            "Epoch 5/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.4232 - precision: 0.8286 - val_loss: 0.1078 - val_precision: 0.9702\n",
            "Epoch 6/6\n",
            "9626/9626 [==============================] - 15s 2ms/step - loss: 0.2132 - precision: 0.9223 - val_loss: 0.0912 - val_precision: 0.9555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdRezmdcmVWF",
        "colab_type": "code",
        "outputId": "ab3fc449-2756-48f5-817d-1bb064cb134b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "print(Scores)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[758.  65.]\n",
            "  [ 45. 738.]]\n",
            "\n",
            " [[764. 255.]\n",
            "  [ 38. 547.]]\n",
            "\n",
            " [[777. 201.]\n",
            "  [ 25. 601.]]\n",
            "\n",
            " [[726.   0.]\n",
            "  [ 76. 802.]]\n",
            "\n",
            " [[790. 182.]\n",
            "  [ 12. 620.]]\n",
            "\n",
            " [[783. 261.]\n",
            "  [ 19. 541.]]\n",
            "\n",
            " [[765.   0.]\n",
            "  [ 37. 802.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn3UL85xUvZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}